{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPivbOeAB5gAnQ9r/831hkW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taruschirag/CollegeInfoTracker/blob/main/Review_Modifer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrn0cSTn37XX",
        "outputId": "9231f504-4a93-449d-b30e-f37e38be8d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textacy\n",
            "  Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/210.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/210.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (5.3.1)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.0.9)\n",
            "Collecting cytoolz>=0.10.1 (from textacy)\n",
            "  Downloading cytoolz-0.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting floret~=0.10.0 (from textacy)\n",
            "  Downloading floret-0.10.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.3/320.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jellyfish>=0.8.0 (from textacy)\n",
            "  Downloading jellyfish-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.3.2)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.23.5)\n",
            "Collecting pyphen>=0.10.0 (from textacy)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.2.2)\n",
            "Requirement already satisfied: spacy~=3.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.6.1)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.10/dist-packages (from textacy) (4.66.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2023.7.22)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->textacy) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.4.7)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (4.7.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy~=3.0->textacy) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy~=3.0->textacy) (2.1.3)\n",
            "Installing collected packages: pyphen, jellyfish, floret, cytoolz, textacy\n",
            "Successfully installed cytoolz-0.12.2 floret-0.10.4 jellyfish-1.0.0 pyphen-0.14.0 textacy-0.13.0\n",
            "['pizza', 'burger', 'pasta']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "!pip install textacy\n",
        "\n",
        "# Load the pre-trained English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "# Define the entity label 'food'\n",
        "LABEL = \"food\"\n",
        "ner = nlp.get_pipe(\"ner\")\n",
        "if not ner:\n",
        "    ner = nlp.create_pipe(\"ner\")\n",
        "    nlp.add_pipe(ner)\n",
        "ner.add_label(LABEL)\n",
        "\n",
        "# Assuming you have a list of food items from the menu\n",
        "menu_items = [\"pizza\", \"burger\", \"pasta\", \"salad\", \"cake\", \"crusty pizza\"\n",
        "              \"lasagna\"]\n",
        "\n",
        "# Generate training data (you can have more complex examples in real scenarios)\n",
        "training_data = []\n",
        "for item in menu_items:\n",
        "    text = f\"I had {item} and it was delicious!\"\n",
        "    doc = nlp.make_doc(text)\n",
        "    entities = [(text.find(item), text.find(item) + len(item), LABEL)]\n",
        "    example = Example.from_dict(doc, {\"entities\": entities})\n",
        "    training_data.append(example)\n",
        "\n",
        "# Train the NER model with the custom food entity\n",
        "n_iter = 10\n",
        "for _ in range(n_iter):\n",
        "    losses = {}\n",
        "    for example in training_data:\n",
        "        nlp.update([example], losses=losses)\n",
        "\n",
        "# Process the customer review\n",
        "review_text = \"I had a pizza and a burger and pasta, both were tasty!\"\n",
        "doc = nlp(review_text)\n",
        "\n",
        "# Extract the food entities\n",
        "food_entities = [ent.text for ent in doc.ents if ent.label_ == LABEL]\n",
        "\n",
        "print(food_entities)  # Output: ['pizza', 'burger']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "import textacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load the pre-trained English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample review text\n",
        "review_text = \"I had pasta and pizza. the ambiance was good. pasta was delicious\"\n",
        "\n",
        "# Process the review text with spaCy\n",
        "doc = nlp(review_text)\n",
        "\n",
        "data = pd.read_excel('Menu_Items.xlsx')  # Replace 'file_path.xlsx' with your actual file path\n",
        "\n",
        "# Extract food entities from the Excel sheet\n",
        "food_entities = data['Food_Items'].tolist()\n",
        "\n",
        "# Extract relevant context sentences mentioning the food entities with positive sentiment\n",
        "food_entities = ['pizza', 'burger', 'pasta']  # Replace with your actual food entities\n",
        "positive_sentiment_words = ['good', 'delicious', 'tasty']  # Add more positive words as needed\n",
        "relevant_context = []\n",
        "\n",
        "for sent in doc.sents:\n",
        "    food_mentioned = any(token.text.lower() in [food.lower() for food in food_entities] for token in sent)\n",
        "    positive_sentiment = any(word in sent.text.lower() for word in positive_sentiment_words)\n",
        "    if food_mentioned and positive_sentiment:\n",
        "        relevant_context.append(sent.text)\n",
        "\n",
        "# Print debug information\n",
        "print(\"Relevant Context Sentences:\")\n",
        "print(relevant_context)\n",
        "\n",
        "# Join the relevant context sentences to form the final extracted text\n",
        "final_extracted_text = \" \".join(relevant_context)\n",
        "\n",
        "print(\"Final Extracted Text:\")\n",
        "print(final_extracted_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofn9IxFg5Hv7",
        "outputId": "d2c3f915-f5d1-4783-ba9e-f8b00aeb3c36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant Context Sentences:\n",
            "['pasta was delicious']\n",
            "Final Extracted Text:\n",
            "pasta was delicious\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "3W1ymWnpEajR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}